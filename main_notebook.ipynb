{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc86acd",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MURA\n",
    "import cv2\n",
    "import image_manipulation\n",
    "from multiprocessing import Pool\n",
    "from models import *\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import json\n",
    "from os import path, mkdir\n",
    "\n",
    "import keras\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22e578b7",
   "metadata": {},
   "source": [
    "# Model to Run\n",
    "This section lets the user edit on what model parameters to run, the model directory and parameters should exist prior to changing the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd350f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All editable variables\n",
    "model_file_path = \"models/model_1\"\n",
    "# If preprocessing needs to run\n",
    "load_model = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc86acd",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "from models import *\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from os import path, mkdir\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b73251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "try:\n",
    "    params = json.load(open(model_file_path + '/parameters.json'))\n",
    "\n",
    "    # Model to use\n",
    "    model_is_VAE = params['is_VAE']\n",
    "    # Model parameters\n",
    "    multiplier = params['multiplier']\n",
    "    latent_size = params['latent_size']\n",
    "    input_shape = params['input_shape']\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    # Dataset Path\n",
    "    dataset_file_path = params['dataset_path']\n",
    "\n",
    "except:\n",
    "    raise Exception(\"No parameters.json file found in the model's directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae204155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array contains the training, validation, and testing in order\n",
    "image_datasets = {'train': []}\n",
    "for dataset_name in image_datasets.keys():\n",
    "    for image_path in glob.glob(f'{dataset_file_path}/{dataset_name}/*.png'):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image_datasets[dataset_name].append(image)\n",
    "    image_datasets[dataset_name] = np.array(image_datasets[dataset_name])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f69e69d0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This section creates and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if model_is_VAE:\n",
    "        model = VAE(False, input_shape, multiplier, latent_size)\n",
    "    else:\n",
    "        model = UPAE(True, input_shape, multiplier, latent_size)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.build(input_shape=(None,) + tuple(input_shape))\n",
    "\n",
    "    model.compile(optimizer= optimizer, loss='mse'\n",
    "                  ,metrics=[tf.keras.metrics.Accuracy()])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    if load_model:\n",
    "        model.load_weights(model_file_path + '/model_weights')\n",
    "    else:\n",
    "        np.random.shuffle(image_datasets['train'])\n",
    "\n",
    "        validation_split = 0.05\n",
    "\n",
    "        total_samples = len(image_datasets['train'])\n",
    "        validation_samples = int(validation_split * total_samples)\n",
    "\n",
    "        train_data = image_datasets['train'][:-validation_samples]\n",
    "        validation_data = image_datasets['train'][-validation_samples:]\n",
    "\n",
    "        # Where images of each epoch will be saved\n",
    "        save_directory = model_file_path + '/callback_images'\n",
    "        save_callback = SaveImageCallback(validation_data, save_directory=save_directory, vae=model_is_VAE)\n",
    "\n",
    "        history_train = model.fit(train_data, \n",
    "                                epochs=epochs, \n",
    "                                batch_size=batch_size,\n",
    "                                validation_data=(validation_data, validation_data),\n",
    "                                callbacks=[save_callback])\n",
    "        \n",
    "        # Saving the model's history\n",
    "        json.dump(history_train.history, open(model_file_path + '/history.json', 'w'))\n",
    "\n",
    "        #Save weights\n",
    "        model.save_weights(model_file_path + '/model_weights', save_format='tf')\n",
    "   \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "076f2a29",
   "metadata": {},
   "source": [
    "# Creating of different Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d99aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving images\n",
    "save_directory = model_file_path + '/plots'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get history of data\n",
    "history = history_train.history\n",
    "\n",
    "def create_plot(history, metric, title, save_name):\n",
    "    plt.plot(history[metric], label=metric)\n",
    "    plt.plot(history[f'val_{metric}'], label=f'val_{metric}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{save_directory}/{save_name}', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Check if UPAE or not\n",
    "if model_is_VAE:\n",
    "    # Create plot for mse_loss\n",
    "    create_plot(history, 'mse_loss', 'MSE Loss per Epoch', 'mse_loss.png')\n",
    "    # Create plot for reconstruction_loss\n",
    "    create_plot(history, 'reconstruction_loss', 'Reconstruction Loss per Epoch', 'reconstruction_loss.png')\n",
    "    # Create plot for kl_loss\n",
    "    create_plot(history, 'kl_loss', 'KL Loss per Epoch', 'kl_loss.png')\n",
    "else:\n",
    "    # Create plot for mse_loss\n",
    "    create_plot(history, 'mse_loss', 'MSE Loss per Epoch', 'mse_loss.png')\n",
    "    # Create plot for total loss\n",
    "    create_plot(history, 'total_loss', 'Total Loss per Epoch', 'total_loss.png')\n",
    "    # Create plot for loss1\n",
    "    create_plot(history, 'loss1', 'Loss 1 per Epoch', 'loss1.png')\n",
    "    # Create plot for loss2\n",
    "    create_plot(history, 'loss2', 'Loss 2 per Epoch', 'loss2.png')\n",
    "    # Create plot for binary_crossentropy\n",
    "    create_plot(history, 'reconstruction_loss', 'Reconstruction Loss per Epoch', 'reconstruction_loss.png')\n",
    "    # Create plot for accuracy\n",
    "    create_plot(history, 'accuracy', 'Accuracy per Epoch', 'accuracy.png')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73f60ee",
   "metadata": {},
   "source": [
    "# Testing of the Model with the Test Set\n",
    "This section tests the model with the current test set\n",
    "TODO: \n",
    "- Get the label of each image in the test set\n",
    "- Test the images\n",
    "- Create Linear Regression for the abnormality score to get the threshold for determining abnormal or normal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels of each image in the image_datasets['test']\n",
    "test_images = []\n",
    "labels = []\n",
    "for image_path in glob.glob(f'{dataset_file_path}/test/*.png'):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Get if it contains positive or negative\n",
    "    if 'positive' in image_path:\n",
    "        test_images.append(image)\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        test_images.append(image)\n",
    "        labels.append(0)\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_images = model.predict(test_images, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for Positives and Negatives of the testing set\n",
    "positive = []\n",
    "negative = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 1:\n",
    "        positive.append(reconstructed_images[1][i])\n",
    "    else:\n",
    "        negative.append(reconstructed_images[1][i])\n",
    "plt.hist(positive, color='red', label='positive', bins=50)\n",
    "plt.hist(negative, color='blue', label='negative', bins=50)\n",
    "# Get x and y labels\n",
    "plt.xlabel('Abnormality Score')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title(\"Abnormality Score and Label Histogram\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{save_directory}/testing_abnormality_scores.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa833ecb",
   "metadata": {},
   "source": [
    "# Getting of best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3753dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(score, threshold):\n",
    "    if score >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_threshold_range(abnormality_scores, num_std):\n",
    "    mean_score = np.mean(abnormality_scores)\n",
    "    std_score = np.std(abnormality_scores)\n",
    "\n",
    "    lower_threshold = mean_score - (num_std * std_score)\n",
    "    upper_threshold = mean_score + (num_std * std_score)\n",
    "\n",
    "    return lower_threshold, upper_threshold\n",
    "\n",
    "def calculate_f1_score(predicted_labels, true_labels):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for predicted, true in zip(predicted_labels, true_labels):\n",
    "        if predicted == 1 and true == 1:\n",
    "            true_positives += 1\n",
    "        elif predicted == 1 and true == 0:\n",
    "            false_positives += 1\n",
    "        elif predicted == 0 and true == 1:\n",
    "            false_negatives += 1\n",
    "        else:\n",
    "            true_negatives += 1\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1_score, precision, recall, true_positives, true_negatives, false_positives, false_negatives\n",
    "\n",
    "def find_optimal_threshold(abnormality_scores, labels):\n",
    "    best_threshold = None\n",
    "    best_metric = 0.0\n",
    "\n",
    "    # Get a range of threshold values\n",
    "    lower_threshold, upper_threshold = get_threshold_range(abnormality_scores, num_std=3)\n",
    "    threshold_range = np.linspace(lower_threshold, upper_threshold, 50)\n",
    "\n",
    "    \n",
    "    # Iterate over a range of threshold values\n",
    "    for threshold in threshold_range:\n",
    "        predicted_labels = [classify_image(score, threshold) for score in abnormality_scores]\n",
    "\n",
    "        # Evaluate performance metric (e.g., accuracy, precision, recall, F1 score)\n",
    "        metric, precision, recall, true_postivies, true_negatives, false_positives, false_negatives = calculate_f1_score(predicted_labels, labels)\n",
    "\n",
    "        # Update the best threshold if the metric improves\n",
    "        if metric > best_metric:\n",
    "            print(\"Current best Threshold: \", threshold, \"\\nCurrent best F1 Score: \", metric)\n",
    "            print(\"Current best Precision: \", precision, \"\\nCurrent best Recall: \", recall)\n",
    "            print(\"\\nCurrent best True Positives: \", true_postivies, \"\\nCurrent best True Negatives: \", true_negatives,\n",
    "                  \"\\nCurrent best False Positives: \", false_positives, \"\\nCurrent best False Negatives: \", false_negatives)\n",
    "            print()\n",
    "            best_metric = metric\n",
    "            best_threshold = threshold\n",
    "\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb40386",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_threshold(np.float32(reconstructed_images[1]), labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f5369b0",
   "metadata": {},
   "source": [
    "# Saving of final reconstructed images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154822a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory in models folder for reconstructed images\n",
    "dataset_name = dataset_file_path.split('/')[-1]\n",
    "reconstructed_images_path = model_file_path + \"/\" + dataset_name\n",
    "if not path.exists(reconstructed_images_path):\n",
    "    mkdir(reconstructed_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597def40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_compared_reconstructed_image(original_image, reconstructed_image, abnormality_score, file_path, image_index,\n",
    "                                      label):\n",
    "    # Put Images in cv2\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_GRAY2BGR)\n",
    "    reconstructed_image = cv2.cvtColor(reconstructed_image, cv2.COLOR_GRAY2BGR)\n",
    "    # Convert reconstructed image to uint8\n",
    "    reconstructed_image = reconstructed_image.astype(np.uint8)\n",
    "    concatenated_img = cv2.hconcat([original_image, reconstructed_image])\n",
    "    # Save the image\n",
    "    cv2.imwrite(f'{file_path}/Image_{image_index}_{abnormality_score}_{label}.png', concatenated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_images)):\n",
    "    save_compared_reconstructed_image(test_images[i], np.array(reconstructed_images[0][i]), reconstructed_images[1][i],\n",
    "                                      reconstructed_images_path, i, labels[i])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d41fb188",
   "metadata": {},
   "source": [
    "# GIF for Callback_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d17c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epoch(filename):\n",
    "    match = re.search(r'epoch_(\\d+)', filename)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "def create_gif(folder_path, output_path, duration=50):\n",
    "    images = []\n",
    "    \n",
    "    # Get all image file names from the folder\n",
    "    filenames = os.listdir(folder_path)\n",
    "    filenames = sorted(filenames, key=lambda x: extract_epoch(x))\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(file_path).convert('P')\n",
    "            images.append(image)\n",
    "    \n",
    "    # Save the images as a GIF\n",
    "    images[0].save(output_path, save_all=True, append_images=images[1:], optimize=False, duration=duration, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_folder_path = model_file_path + \"/\" + \"callback_images\"\n",
    "image_folders = ['image0', 'image1', 'image2', 'image3']\n",
    "\n",
    "for i,folder in enumerate(image_folders):\n",
    "    curr_folder = gif_folder_path + \"/\" + folder\n",
    "    gif_output_path = gif_folder_path + \"/\" + f\"image{i}.gif\"\n",
    "    print(f\"Creating GIF for: image{i}\")\n",
    "    create_gif(curr_folder, gif_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
