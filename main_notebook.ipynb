{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc86acd",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b7f7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SKill\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import MURA\n",
    "import cv2\n",
    "import image_manipulation\n",
    "from multiprocessing import Pool\n",
    "from models import VAE, UPAE, SaveImageCallback\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import json\n",
    "from os import path, mkdir\n",
    "\n",
    "import keras\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22e578b7",
   "metadata": {},
   "source": [
    "# Model to Run\n",
    "This section lets the user edit on what model parameters to run, the model directory and parameters should exist prior to changing the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd350f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All editable variables\n",
    "model_file_path = \"models/model_2\"\n",
    "# If preprocessing needs to run\n",
    "run_preprocessing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b73251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "try:\n",
    "    params = json.load(open(model_file_path + '/parameters.json'))\n",
    "\n",
    "    # Model parameters\n",
    "    multiplier = params['multiplier']\n",
    "    latent_size = params['latent_size']\n",
    "    input_shape = params['input_shape']\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    # Dataset Path\n",
    "    image_paths = MURA.MURA_DATASET()\n",
    "    dataset_file_path = params['dataset_path']\n",
    "    all_image_paths = image_paths.get_combined_image_paths()\n",
    "    all_image_paths = all_image_paths.to_numpy()[:,0]\n",
    "except:\n",
    "    raise Exception(\"No parameters.json file found in the model's directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd25378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do preprocessing\n",
    "if run_preprocessing:\n",
    "    preprocess = preprocessing.preprocessing(input_path = all_image_paths, output_path = dataset_file_path)\n",
    "    if __name__ == '__main__':\n",
    "        preprocess.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae204155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array contains the training, validation, and testing in order\n",
    "image_datasets = {'train': [],\n",
    "                'valid': [],\n",
    "                'test': []}\n",
    "for dataset_name in image_datasets.keys():\n",
    "    for image_path in glob.glob(f'{dataset_file_path}/{dataset_name}/*.png'):\n",
    "        image_datasets[dataset_name].append(cv2.imread(image_path))\n",
    "    image_datasets[dataset_name] = np.array(image_datasets[dataset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128538d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'][0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f69e69d0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This section creates and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f906dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 256)         524544    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 256)         1048832   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      1048832   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 32, 32, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 128)      524416    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 64)       131136    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 64, 64, 3)        3075      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " encoder (Sequential)        (None, 4, 4, 256)         1710528   \n",
      "                                                                 \n",
      " latent_space_representation  (None, 16, 16, 256)      143201408 \n",
      "  (Sequential)                                                   \n",
      "                                                                 \n",
      " decoder (Sequential)        (None, 64, 64, 3)         1709251   \n",
      "                                                                 \n",
      " z_mean (Dense)              multiple                  16448     \n",
      "                                                                 \n",
      " z_log_var (Dense)           multiple                  16448     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146,654,083\n",
      "Trainable params: 146,651,267\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "Vanilla Loss\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['z_mean/kernel:0', 'z_mean/bias:0', 'z_log_var/kernel:0', 'z_log_var/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "Vanilla Loss\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['z_mean/kernel:0', 'z_mean/bias:0', 'z_log_var/kernel:0', 'z_log_var/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "193/193 [==============================] - 20s 77ms/step - mse_loss: 2238.2183 - reconstruction_loss: -3236043.5000 - kl_loss: inf\n",
      "Epoch 2/5\n",
      "193/193 [==============================] - 15s 76ms/step - mse_loss: 771.8772 - reconstruction_loss: -3756667.7500 - kl_loss: inf\n",
      "Epoch 3/5\n",
      "193/193 [==============================] - 15s 80ms/step - mse_loss: 573.4001 - reconstruction_loss: -3748895.2500 - kl_loss: inf\n",
      "Epoch 4/5\n",
      "193/193 [==============================] - 17s 87ms/step - mse_loss: 474.5878 - reconstruction_loss: -3744997.5000 - kl_loss: inf\n",
      "Epoch 5/5\n",
      "193/193 [==============================] - 14s 73ms/step - mse_loss: 446.2208 - reconstruction_loss: -3745615.0000 - kl_loss: inf\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #for either VAE or UPAE\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--u', dest='u', action='store_true') # use uncertainty\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "\n",
    "    #preprocessing and augmentation\n",
    "    # image_datasets = data_preparation()\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    if opt.u is False:\n",
    "        model = VAE(opt.u, input_shape, multiplier, latent_size)\n",
    "    elif opt.u is True:\n",
    "        model = UPAE(opt.u, input_shape, multiplier, latent_size)\n",
    "\n",
    "    model.build(input_shape=(None, 64, 64, 3))\n",
    "\n",
    "    model.compile(optimizer= optimizer\n",
    "                  ,metrics=[tf.keras.metrics.Accuracy()])\n",
    "    \n",
    "    # Where images of each epoch will be saved\n",
    "    save_directory = 'Images/images_epochs'\n",
    "    save_callback = SaveImageCallback(image_datasets[0], save_directory)\n",
    "\n",
    "    # plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
    "    #training on training set.\n",
    "    history_train = model.fit(image_datasets['train'], \n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96a1d3c",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7071dba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_valid \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(image_datasets[\u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m], batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[1;32md:\\GithubRepos\\MURA-Classification\\models.py:197\u001b[0m, in \u001b[0;36mVAE.predict\u001b[1;34m(self, data, batch_size)\u001b[0m\n\u001b[0;32m    195\u001b[0m     abnormal_score \u001b[39m=\u001b[39m (reconstruction\u001b[39m-\u001b[39mbatch_data) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    196\u001b[0m     abnormal_score \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(abnormal_score, axis\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[1;32m--> 197\u001b[0m     predictions\u001b[39m.\u001b[39;49mappend(reconstruction, abnormal_score)\n\u001b[0;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(predictions, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "history_valid = model.predict(image_datasets['valid'], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78a2533e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_valid\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_valid' is not defined"
     ]
    }
   ],
   "source": [
    "history_valid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "353efa2a",
   "metadata": {},
   "source": [
    "# Plots Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b363b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss = history_train.history['binary_crossentropy: ']\n",
    "# # valid_loss = history_valid.history['binary_crossentropy: ']\n",
    "\n",
    "# plt.plot(train_loss , label='train')\n",
    "# # plt.plot(valid_loss , label='test')\n",
    "# plt.title('Train vs Validation Loss')\n",
    "# plt.ylabel('Reconstruction Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73f60ee",
   "metadata": {},
   "source": [
    "# Testing of the Model with the Test Set\n",
    "This section tests the model with the current test set\n",
    "TODO: \n",
    "- Get the label of each image in the test set\n",
    "- Test the images\n",
    "- Create Linear Regression for the abnormality score to get the threshold for determining abnormal or normal images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f5369b0",
   "metadata": {},
   "source": [
    "# Saving of final reconstructed images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154822a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory in models folder for reconstructed images\n",
    "dataset_name = dataset_file_path.split('/')[-1]\n",
    "reconstructed_images_path = model_file_path + \"/\" + dataset_name\n",
    "if not path.exists(reconstructed_images_path):\n",
    "    mkdir(reconstructed_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7d13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(history_valid)):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "    axs[0].imshow(image_datasets['valid'][x])\n",
    "    axs[0].set_title('Original Image')\n",
    "    new_image = np.floor(history_valid[x]).astype(np.uint8)\n",
    "    axs[1].imshow(new_image)\n",
    "    axs[1].set_title('Reconstructed Image')\n",
    "    plt.savefig(f'{reconstructed_images_path}/Valid_Image_{x}.png')\n",
    "    plt.close()\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72b430df",
   "metadata": {},
   "source": [
    "# Saving of Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324861d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_file_path + '/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_valid[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46067ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = np.concatenate(history_valid[0], axis=1)\n",
    "plt.imshow(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_valid[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90fc7ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
